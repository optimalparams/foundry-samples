{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9290f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748813b",
   "metadata": {},
   "source": [
    "# Mistral Document AI Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d704f0",
   "metadata": {},
   "source": [
    "In addition to basic OCR functionality, Mistral Document AI has annotations that allow you to extract information from documents and images in structured json with a single call to the API. It offers two types of annotations that can be used independently, or together:\n",
    "\n",
    "- bbox_annotation: gives you the annotation of the bounding boxes extracted by the OCR model (charts/figures etc.) based on a structure that you define in the request. This is provided for every image, in every page of the document.\n",
    "- document_annotation: like the bbox_annotation, but for the extracted text across the entire document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f024dee",
   "metadata": {},
   "source": [
    "> **Note**: Document annotations are currently limited at 8 pages. Please see the model card for the most up-to-date information on limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e0281",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0172af",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_MISTRAL_DOCUMENT_AI_ENDPOINT = \"\"\n",
    "AZURE_MISTRAL_DOCUMENT_AI_KEY = \"\"\n",
    "REQUEST_HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {AZURE_MISTRAL_DOCUMENT_AI_KEY}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f536be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/mistral7b.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcff75f",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706550d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path: str) -> str:\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {image_path} was not found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187390a",
   "metadata": {},
   "source": [
    "# 2. Bounding Box Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbd992",
   "metadata": {},
   "source": [
    "In this example we will define a `bbox_annotation_format` as part of our request using JSON schema. In it, we will have fields for the type of image, and a short description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1329cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedDocument = encode_image(\"../images/mistral7b.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxannotationPayload = {\n",
    "    \"model\": \"mistral-document-ai-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": f\"data:application/pdf;base64,{encodedDocument}\",\n",
    "    },\n",
    "    \"include_image_base64\": \"true\",\n",
    "    \"bbox_annotation_format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"schema\": {\n",
    "                \"properties\": {\n",
    "                    \"image_type\": {\n",
    "                        \"description\": \"The type of the image.\",\n",
    "                        \"title\": \"Image Type\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                    \"short_description\": {\n",
    "                        \"description\": \"A description in english describing the image.\",\n",
    "                        \"title\": \"Short Description\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb1Response = requests.post(\n",
    "    url=AZURE_MISTRAL_DOCUMENT_AI_ENDPOINT,\n",
    "    json=bboxannotationPayload,\n",
    "    headers=REQUEST_HEADERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d74ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in bb1Response.json()[\"pages\"]:\n",
    "    for image in page[\"images\"]:\n",
    "        print(\"page \" + str(page[\"index\"]))\n",
    "        iaj = json.loads(image[\"image_annotation\"])\n",
    "        print(\"Image type: \" + iaj[\"properties\"][\"image_type\"])\n",
    "        print(\"Short description: \" + iaj[\"properties\"][\"short_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f647809",
   "metadata": {},
   "source": [
    "And here we see that we get the `image_annotation` in the response for each image in the document. It contains the fields we defined in the request for predictable extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa899a",
   "metadata": {},
   "source": [
    "# 3. Document Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39776ca0",
   "metadata": {},
   "source": [
    "Building off the bbox_annotations, we can add a `document_annotation_format` to our request. Just like the bbox_annotations, we define this in JSON schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comboAnnotationPayload = {\n",
    "    \"model\": \"mistral-document-ai-2505\",\n",
    "    \"pages\": [0, 1, 2, 3],\n",
    "    \"document\": {\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": f\"data:application/pdf;base64,{encodedDocument}\",\n",
    "    },\n",
    "    \"include_image_base64\": \"true\",\n",
    "    \"bbox_annotation_format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"schema\": {\n",
    "                \"properties\": {\n",
    "                    \"image_type\": {\n",
    "                        \"description\": \"The type of the image.\",\n",
    "                        \"title\": \"Image Type\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                    \"short_description\": {\n",
    "                        \"description\": \"A description in english describing the image.\",\n",
    "                        \"title\": \"Short Description\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"document_annotation_format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"schema\": {\n",
    "                \"properties\": {\n",
    "                    \"language\": {\n",
    "                        \"title\": \"Language\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The language of the document.\",\n",
    "                    },\n",
    "                    \"summary\": {\n",
    "                        \"title\": \"Summary\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A brief summary of the document in English.\",\n",
    "                    },\n",
    "                    \"chapter_titles\": {\n",
    "                        \"title\": \"Chapter_Titles\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The titles of the chapters in the document.\",\n",
    "                    },\n",
    "                    \"urls\": {\n",
    "                        \"title\": \"urls\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The urls in the document.\",\n",
    "                    },\n",
    "                    \"translated_summary\": {\n",
    "                        \"title\": \"Translation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The French translation of the document summary.\",\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comboResponse = requests.post(\n",
    "    url=AZURE_MISTRAL_DOCUMENT_AI_ENDPOINT,\n",
    "    json=comboAnnotationPayload,\n",
    "    headers=REQUEST_HEADERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "docAnnotation = json.loads(comboResponse.json()[\"document_annotation\"])\n",
    "print(\"Language: \" + docAnnotation[\"properties\"][\"language\"])\n",
    "print(\"Summary: \" + docAnnotation[\"properties\"][\"summary\"])\n",
    "print(\"Chapter Titles: \" + docAnnotation[\"properties\"][\"chapter_titles\"])\n",
    "print(\"URLs: \" + docAnnotation[\"properties\"][\"urls\"])\n",
    "print(\"Translated Summary: \" + docAnnotation[\"properties\"][\"translated_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd80b9",
   "metadata": {},
   "source": [
    "And here we see the returned document annotations as we prescribed in the request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0b18e",
   "metadata": {},
   "source": [
    "# 4. Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e126d35",
   "metadata": {},
   "source": [
    "Being able to extract text and images from documents is powerful, when you combine this with structured extraction and enrichment it grants you the ability to create powerful document processing and intelligence capabilities. We hope you found this notebook useful, and look forward to seeing what you build with Mistral Document AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
